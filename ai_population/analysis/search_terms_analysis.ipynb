{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path) -> list:\n",
    "    \"\"\"\n",
    "    Load search terms for market signals or profile list from text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file containing search terms/profiles, one per line.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of search terms/profiles as strings.\n",
    "    \"\"\"\n",
    "    full_file_path = f\"../config/{file_path}\"\n",
    "    with open(full_file_path, \"r\") as file:\n",
    "        return [line.strip() for line in file]\n",
    "    \n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "\n",
    "def combine_metadata_text(row: pd.Series) -> str:\n",
    "    combined_text_list = []\n",
    "\n",
    "    # Append video text\n",
    "    if row[\"text\"] is not None and not pd.isnull(row[\"text\"]):\n",
    "        combined_text_list.append(row[\"text\"])\n",
    "\n",
    "    # Append video transcript\n",
    "    if row[\"video_transcript\"] is not None and not pd.isnull(row[\"video_transcript\"]):\n",
    "        combined_text_list.append(row[\"video_transcript\"])\n",
    "\n",
    "    # Append video hashtags\n",
    "    if row[\"hashtags\"] is not None and not pd.isnull(row[\"hashtags\"]):\n",
    "        hashtag_str = row[\"hashtags\"].replace(\"'\", '\"')\n",
    "        hashtag_list = json.loads(hashtag_str)\n",
    "        hashtag_value_list = [d['name'] for d in hashtag_list]\n",
    "        combined_text_list += hashtag_value_list \n",
    "        \n",
    "    if combined_text_list == []:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return \"\\n\".join(combined_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_video_metadata = pd.read_csv(\"../data/market-signals-finfluencer/profilesearch_video_metadata_identification.csv\")\n",
    "\n",
    "# Identify financial influencers\n",
    "groundtruth_finfluencers = load_text_file(\"market_signals_finfluencer_profiles_finfluencers.txt\")\n",
    "print(f\"Number of Financial Influencers: {len(groundtruth_finfluencers)}\")\n",
    "\n",
    "# Identify financial influencers whose primary focus is on stocks trading and equities, bonds and fixed income, and options trading and derivatives\n",
    "identification_results = pd.read_csv(\"../data/market-signals-finfluencer/profile_metadata_post_identification.csv\")\n",
    "filtered_identification_results = identification_results[identification_results[\"Which of these areas of finance are the primary focus of the influencerâ€™s posts? - symbol\"].str.contains('B1|B2|B3', na=False)]\n",
    "relevant_finfluencers = [profile for profile in groundtruth_finfluencers if profile in filtered_identification_results[\"profile\"].tolist()]\n",
    "print(f\"Number of Financial Influencers in B1, B2, and B3: {len(relevant_finfluencers)}\")\n",
    "\n",
    "finfluencer_video_metadata = combined_video_metadata[combined_video_metadata[\"profile\"].isin(relevant_finfluencers)].reset_index(drop=True)\n",
    "print(finfluencer_video_metadata.shape)\n",
    "finfluencer_video_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the text columns: 'text', 'hashtags', and 'video_transcript'\n",
    "finfluencer_video_metadata['combined_text'] = finfluencer_video_metadata.apply(combine_metadata_text, axis=1)\n",
    "\n",
    "# Preprocess the text: lowercase and remove punctuation\n",
    "finfluencer_video_metadata['combined_text'] = finfluencer_video_metadata['combined_text'].apply(preprocess)\n",
    "\n",
    "finfluencer_video_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare stop words list\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Initialize TfidfVectorizer with n-gram range 1 to 3\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(finfluencer_video_metadata['combined_text'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the TF-IDF scores for each n-gram across all documents\n",
    "tfidf_sum = np.sum(tfidf_matrix.toarray(), axis=0)\n",
    "keywords_scores = list(zip(feature_names, tfidf_sum))\n",
    "\n",
    "# Separate the keywords by their n-gram length\n",
    "unigrams = [(kw, score) for kw, score in keywords_scores if len(kw.split()) == 1]\n",
    "bigrams  = [(kw, score) for kw, score in keywords_scores if len(kw.split()) == 2]\n",
    "trigrams = [(kw, score) for kw, score in keywords_scores if len(kw.split()) == 3]\n",
    "\n",
    "# Sort each list in descending order based on the aggregated score\n",
    "unigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "bigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "trigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Define how many top results to show\n",
    "top_n = 50\n",
    "top_unigrams = unigrams[:top_n]\n",
    "top_bigrams  = bigrams[:top_n]\n",
    "top_trigrams = trigrams[:top_n]\n",
    "\n",
    "print(\"Top 50 Unigrams (TF-IDF):\")\n",
    "for kw, score in top_unigrams:\n",
    "    print(f\"{kw}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 50 Bigrams (TF-IDF):\")\n",
    "for kw, score in top_bigrams:\n",
    "    print(f\"{kw}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 50 Trigrams (TF-IDF):\")\n",
    "for kw, score in top_trigrams:\n",
    "    print(f\"{kw}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Word Analysis of Financial Influencers vs Non-Financial Influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path) -> list:\n",
    "    \"\"\"\n",
    "    Load search terms for market signals or profile list from text file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file containing search terms/profiles, one per line.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of search terms/profiles as strings.\n",
    "    \"\"\"\n",
    "    full_file_path = f\"../config/{file_path}\"\n",
    "    with open(full_file_path, \"r\") as file:\n",
    "        return [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadata = pd.read_csv(\"../data/market-signals-finfluencer/profilesearch_video_metadata_identification.csv\")\n",
    "\n",
    "finfluencer_list = load_text_file(\"market_signals_finfluencer_profiles_finfluencers.txt\")\n",
    "nonfinfluencer_list = load_text_file(\"market_signals_finfluencer_profiles_nonfinfluencers.txt\")\n",
    "\n",
    "video_metadata[\"Finfluencer\"] = video_metadata[\"profile\"].apply(lambda x: 1 if x in finfluencer_list else 0)\n",
    "\n",
    "finfluencer_video_metadata = video_metadata[video_metadata[\"Finfluencer\"] == 1].reset_index(drop=True)\n",
    "nonfinfluencer_video_metadata = video_metadata[video_metadata[\"Finfluencer\"] != 1].reset_index(drop=True)\n",
    "\n",
    "print(video_metadata.shape)\n",
    "video_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to remove stop words from text\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def generate_word_cloud(combined_text: str, title: str) -> None:\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(combined_text)\n",
    "\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate word cloud for financial influencers\n",
    "finfluencer_video_text = \" \".join(finfluencer_video_metadata[\"text\"].dropna())\n",
    "finfluencer_transcripts = \" \".join(finfluencer_video_metadata[\"video_transcript\"].dropna())\n",
    "combined_finfluencer_text = remove_stopwords(finfluencer_video_text + \" \" + finfluencer_transcripts)\n",
    "generate_word_cloud(combined_finfluencer_text, \"Word Cloud for Financial Influencers\")\n",
    "\n",
    "# Generate word cloyd for non-financial influencers\n",
    "nonfinfluencer_video_text = \" \".join(nonfinfluencer_video_metadata[\"text\"].dropna())\n",
    "nonfinfluencer_transcripts = \" \".join(nonfinfluencer_video_metadata[\"video_transcript\"].dropna())\n",
    "combined_nonfinfluencer_text = remove_stopwords(nonfinfluencer_video_text + \" \" + nonfinfluencer_transcripts)\n",
    "generate_word_cloud(combined_nonfinfluencer_text, \"Word Cloud for Non-Financial Influencers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-signals-tiktok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
