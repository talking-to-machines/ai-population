{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set working directory to project root, if not done already.\n",
    "project_root = Path('/Users/raymondlow/Documents/talking-to-machines/ai-population').resolve()\n",
    "os.chdir(project_root)\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set __package__ so that relative imports work.\n",
    "__package__ = \"ai_population.analysis\"\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "PROJECT_NAME = \"market-signals-x\"\n",
    "EXECUTION_DATE = \"ground-truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "\n",
    "def combine_metadata_text(row: pd.Series) -> str:\n",
    "    combined_text_list = []\n",
    "\n",
    "    # Append tweet text\n",
    "    if row[\"text\"] is not None and not pd.isnull(row[\"text\"]):\n",
    "        combined_text_list.append(row[\"text\"])\n",
    "\n",
    "    if row[\"hashtags\"] is not None and not pd.isnull(row[\"hashtags\"]):\n",
    "        combined_text_list += row[\"hashtags\"].split(\", \")\n",
    "        \n",
    "    if combined_text_list == []:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return preprocess(\"\\n\".join(combined_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify financial influencers\n",
    "ground_truth_profiles = pd.read_csv(os.path.join(\"ai_population/data\", PROJECT_NAME, EXECUTION_DATE, \"ground_truth_profile_list.csv\"))\n",
    "ground_truth_finfluencers = ground_truth_profiles[ground_truth_profiles[\"finfluencer\"] == \"Yes\"][\"account_id\"].tolist()\n",
    "print(f\"Number of Financial Influencers: {len(ground_truth_finfluencers)}\")\n",
    "ground_truth_nonfinfluencers = ground_truth_profiles[ground_truth_profiles[\"finfluencer\"] == \"No\"][\"account_id\"].tolist()\n",
    "print(f\"Number of Non-Financial Influencers: {len(ground_truth_nonfinfluencers)}\")\n",
    "\n",
    "# Identify financial influencers whose primary focus is on stocks trading and equities, bonds and fixed income, and options trading and derivatives\n",
    "post_data = pd.read_csv(os.path.join(\"ai_population/data\", PROJECT_NAME, EXECUTION_DATE, \"ground_truth_profile_posts.csv\"))\n",
    "finfluencer_post_data = post_data[post_data[\"account_id\"].isin(ground_truth_finfluencers)].reset_index(drop=True)\n",
    "nonfinfluencer_post_data = post_data[post_data[\"account_id\"].isin(ground_truth_nonfinfluencers)].reset_index(drop=True)\n",
    "\n",
    "print(finfluencer_post_data.shape)\n",
    "print(nonfinfluencer_post_data.shape)\n",
    "finfluencer_post_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finfluencer_post_data['combined_text'] = finfluencer_post_data.apply(combine_metadata_text, axis=1)\n",
    "nonfinfluencer_post_data['combined_text'] = nonfinfluencer_post_data.apply(combine_metadata_text, axis=1)\n",
    "finfluencer_post_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Analysis with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tfidf_analysis(video_data: pd.DataFrame) -> None:\n",
    "    # Prepare stop words list\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Initialize TfidfVectorizer with n-gram range 1 to 3\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(video_data['combined_text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Sum the TF-IDF scores for each n-gram across all documents\n",
    "    tfidf_sum = np.sum(tfidf_matrix.toarray(), axis=0)\n",
    "    keywords_scores = list(zip(feature_names, tfidf_sum))\n",
    "\n",
    "    # Separate the keywords by their n-gram length\n",
    "    unigrams = [(kw, score) for kw, score in keywords_scores if len(kw.split()) == 1]\n",
    "    bigrams  = [(kw, score) for kw, score in keywords_scores if len(kw.split()) == 2]\n",
    "    trigrams = [(kw, score) for kw, score in keywords_scores if len(kw.split()) == 3]\n",
    "\n",
    "    # Sort each list in descending order based on the aggregated score\n",
    "    unigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "    bigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "    trigrams.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Define how many top results to show\n",
    "    top_n = 50\n",
    "    top_unigrams = unigrams[:top_n]\n",
    "    top_bigrams  = bigrams[:top_n]\n",
    "    top_trigrams = trigrams[:top_n]\n",
    "\n",
    "    return top_unigrams, top_bigrams, top_trigrams\n",
    "\n",
    "\n",
    "def remove_overlapping_keywords(finfluencer_keywords: dict, nonfinfluencer_keywords: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Remove overlapping keywords from the finfluencer keywords.\n",
    "    \"\"\"\n",
    "    finfluencer_keywords_set = set([kw for kw, _ in finfluencer_keywords])\n",
    "    nonfinfluencer_keywords_set = set([kw for kw, _ in nonfinfluencer_keywords])\n",
    "\n",
    "    # Find overlapping keywords\n",
    "    overlapping_keywords = finfluencer_keywords_set.intersection(nonfinfluencer_keywords_set)\n",
    "    print(f\"Overlapping Keywords: {overlapping_keywords}\")\n",
    "\n",
    "    # Remove overlapping keywords from finfluencer keywords\n",
    "    filtered_finfluencer_keywords = [(kw, score) for kw, score in finfluencer_keywords if kw not in overlapping_keywords]\n",
    "\n",
    "    return filtered_finfluencer_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_finfluencer_unigrams, top_finfluencer_bigrams, top_finfluencer_trigrams = perform_tfidf_analysis(finfluencer_post_data)\n",
    "top_nonfinfluencer_unigrams, top_nonfinfluencer_bigrams, top_nonfinfluencer_trigrams = perform_tfidf_analysis(nonfinfluencer_post_data)\n",
    "\n",
    "processed_unigrams = remove_overlapping_keywords(top_finfluencer_unigrams, top_nonfinfluencer_unigrams)\n",
    "processed_bigrams = remove_overlapping_keywords(top_finfluencer_bigrams, top_nonfinfluencer_bigrams)\n",
    "processed_trigrams = remove_overlapping_keywords(top_finfluencer_trigrams, top_nonfinfluencer_trigrams)\n",
    "\n",
    "\n",
    "print(\"Top 50 Unigrams (TF-IDF):\")\n",
    "for kw, score in processed_unigrams:\n",
    "    print(f\"{kw}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 50 Bigrams (TF-IDF):\")\n",
    "for kw, score in processed_bigrams:\n",
    "    print(f\"{kw}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 50 Trigrams (TF-IDF):\")\n",
    "for kw, score in processed_trigrams:\n",
    "    print(f\"{kw}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Word Analysis of Financial Influencers vs Non-Financial Influencers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to remove stop words from text\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def generate_word_cloud(combined_text: str, title: str) -> None:\n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(combined_text)\n",
    "\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Generate word cloud for financial influencers\n",
    "finfluencer_text = \"\\n\".join(finfluencer_post_data[\"combined_text\"].dropna())\n",
    "generate_word_cloud(finfluencer_text, \"Word Cloud for Financial Influencers\")\n",
    "\n",
    "# Generate word cloyd for non-financial influencers\n",
    "nonfinfluencer_text = \"\\n\".join(nonfinfluencer_post_data[\"combined_text\"].dropna())\n",
    "generate_word_cloud(nonfinfluencer_text, \"Word Cloud for Non-Financial Influencers\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market-signals-tiktok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
